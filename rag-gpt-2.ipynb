{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-22T19:51:34.242204Z","iopub.execute_input":"2025-03-22T19:51:34.242560Z","iopub.status.idle":"2025-03-22T19:51:34.247295Z","shell.execute_reply.started":"2025-03-22T19:51:34.242538Z","shell.execute_reply":"2025-03-22T19:51:34.246125Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"pip install faiss-cpu","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T19:52:26.242110Z","iopub.execute_input":"2025-03-22T19:52:26.242468Z","iopub.status.idle":"2025-03-22T19:52:33.213338Z","shell.execute_reply.started":"2025-03-22T19:52:26.242448Z","shell.execute_reply":"2025-03-22T19:52:33.212101Z"}},"outputs":[{"name":"stdout","text":"Collecting faiss-cpu\n  Downloading faiss_cpu-1.10.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (4.4 kB)\nRequirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.26.4)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<3.0,>=1.25.0->faiss-cpu) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\nDownloading faiss_cpu-1.10.0-cp310-cp310-manylinux_2_28_x86_64.whl (30.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: faiss-cpu\nSuccessfully installed faiss-cpu-1.10.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import torch\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer\nimport faiss\nimport numpy as np\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T19:52:42.146193Z","iopub.execute_input":"2025-03-22T19:52:42.146580Z","iopub.status.idle":"2025-03-22T19:52:42.185024Z","shell.execute_reply.started":"2025-03-22T19:52:42.146560Z","shell.execute_reply":"2025-03-22T19:52:42.183492Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel\n\n# Load GPT-2 tokenizer and model\nmodel_name = \"gpt2\"\ntokenizer = GPT2Tokenizer.from_pretrained(model_name)\nmodel = GPT2LMHeadModel.from_pretrained(model_name)\n\n# Set padding token (Fix for ValueError)\ntokenizer.pad_token = tokenizer.eos_token  # Use EOS token as padding\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T19:52:44.824137Z","iopub.execute_input":"2025-03-22T19:52:44.824490Z","iopub.status.idle":"2025-03-22T19:52:58.630576Z","shell.execute_reply.started":"2025-03-22T19:52:44.824470Z","shell.execute_reply":"2025-03-22T19:52:58.629632Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"05793c34b4d44a0786b0a5725c6907b6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb65e9a4710a40bb94249d166127e5db"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb19855935d94a4f9e083a562ff20f45"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"359ce05be4b64e4bba71e96751132e66"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3547948d7c747baa1f411b6896569fe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"541c99ca596b40ed9d9fb6b9cd8f9daf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f23d8a89a5da42a09bb5b86e50efa27f"}},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"input_text = \"Hey\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n\noutput = model.generate(input_ids, max_length=100, do_sample=True, temperature=0.7)\ngenerated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n\nprint(generated_text)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T19:54:34.300095Z","iopub.execute_input":"2025-03-22T19:54:34.300435Z","iopub.status.idle":"2025-03-22T19:54:38.440156Z","shell.execute_reply.started":"2025-03-22T19:54:34.300401Z","shell.execute_reply":"2025-03-22T19:54:38.439221Z"}},"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nThe attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","output_type":"stream"},{"name":"stdout","text":"Hey, now that the whole thing has been done, you're gonna be able to say that this was going to be my first real attempt at writing and I was like, 'Ah, that's cool.' Yeah, yeah, yeah. But then I read the first couple of pages of it and it became like, 'Oh, yeah. Well, that's what I'd want to do. I'd like to do something different.' But then I read the third page of it and it became\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"documents = [\n    \"GPT-2 is a transformer model for text generation.\",\n    \"RAG combines retrieval with generation to improve LLM responses.\",\n    \"FAISS is used for efficient similarity search.\",\n    \"Nasro is one of the best ai engineers on earth\"\n]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T19:56:36.893312Z","iopub.execute_input":"2025-03-22T19:56:36.893619Z","iopub.status.idle":"2025-03-22T19:56:36.897279Z","shell.execute_reply.started":"2025-03-22T19:56:36.893601Z","shell.execute_reply":"2025-03-22T19:56:36.896410Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"def encode_documents(docs, tokenizer, model):\n    embeddings = []\n    for doc in docs:\n        tokens = tokenizer(doc, return_tensors=\"pt\", truncation=True, padding=True)  # Padding is now fixed\n        with torch.no_grad():\n            outputs = model.transformer.wte(tokens.input_ids).mean(dim=1)  # Mean of token embeddings\n        embeddings.append(outputs.squeeze().numpy())\n    return np.array(embeddings)\n\ndoc_embeddings = encode_documents(documents, tokenizer, model)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T19:56:38.611888Z","iopub.execute_input":"2025-03-22T19:56:38.612201Z","iopub.status.idle":"2025-03-22T19:56:38.622162Z","shell.execute_reply.started":"2025-03-22T19:56:38.612178Z","shell.execute_reply":"2025-03-22T19:56:38.620940Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"import faiss\n\n# Convert embeddings to float32 (FAISS requires this format)\ndoc_embeddings = np.array(doc_embeddings).astype('float32')\n\n# Create FAISS index\nindex = faiss.IndexFlatL2(doc_embeddings.shape[1])  # L2 distance for similarity search\nindex.add(doc_embeddings)\n\n# Store document texts for retrieval\ndoc_texts = documents","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T19:56:40.266083Z","iopub.execute_input":"2025-03-22T19:56:40.266347Z","iopub.status.idle":"2025-03-22T19:56:40.270892Z","shell.execute_reply.started":"2025-03-22T19:56:40.266330Z","shell.execute_reply":"2025-03-22T19:56:40.269808Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"def retrieve_top_document(query, tokenizer, model, index, doc_texts, top_k=1):\n    # Encode query into embeddings\n    query_embedding = encode_documents([query], tokenizer, model)[0].astype('float32')\n    \n    # Search FAISS index for the most similar document\n    _, indices = index.search(np.array([query_embedding]), top_k)\n    \n    # Return the top document\n    return doc_texts[indices[0][0]]\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T19:56:41.708008Z","iopub.execute_input":"2025-03-22T19:56:41.708293Z","iopub.status.idle":"2025-03-22T19:56:41.713239Z","shell.execute_reply.started":"2025-03-22T19:56:41.708274Z","shell.execute_reply":"2025-03-22T19:56:41.712122Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# Test retrieval\nquery = \"Who is Nasro?\"\nretrieved_doc = retrieve_top_document(query, tokenizer, model, index, doc_texts)\nprint(f\"Retrieved Document: {retrieved_doc}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T19:57:02.842341Z","iopub.execute_input":"2025-03-22T19:57:02.842661Z","iopub.status.idle":"2025-03-22T19:57:02.848301Z","shell.execute_reply.started":"2025-03-22T19:57:02.842643Z","shell.execute_reply":"2025-03-22T19:57:02.847451Z"}},"outputs":[{"name":"stdout","text":"Retrieved Document: Nasro is one of the best ai engineers on earth\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}